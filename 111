import matplotlib.pyplot as plt
import random

from matplotlib.colors import ListedColormap
from sklearn import datasets

import numpy as np
from sklearn import model_selection
from sklearn import tree

# сгенерируем данные
classification_data, classification_labels = datasets.make_classification(
    n_samples=100,
    n_features=2, n_informative=2,
    n_classes=2, n_redundant=0,
    n_clusters_per_class=1,
    random_state=5
)


# Класс узла

class Node:

    def __init__(self, index, t, true_branch, false_branch):
        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле
        self.t = t  # значение порога
        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле
        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле


# Класс терминального узла (листа)

class Leaf:

    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        self.prediction = self.predict()
        self.leaves_count = 0

    def predict(self):
        # подсчет количества объектов разных классов
        classes = {}  # сформируем словарь "класс: количество объектов"
        for label in self.labels:
            if label not in classes:
                classes[label] = 0
            classes[label] += 1
        #  найдем класс, количество объектов которого будет максимальным в этом листе и вернем его
        prediction = max(classes, key=classes.get)
        return prediction

    # Класс дерева


class MyDecisionTree:

    def __init__(self, min_leaf=2, max_depth=None, max_leaves=None):
        self.min_leaf = min_leaf
        self.max_depth = max_depth
        self.max_leaves = max_leaves
        self.leaves_count = 0

    # Расчет критерия Джини
    def gini(self, labels):
        #  подсчет количества объектов разных классов
        classes = {}
        for label in labels:
            if label not in classes:
                classes[label] = 0
            classes[label] += 1

        #  расчет критерия
        impurity = 1  # коэффициент неопределенности Джини
        for label in classes:
            p = classes[label] / len(labels)
            impurity -= p ** 2

        return impurity

    # Расчет качества
    def quality(self, left_labels, right_labels, current_gini):

        # доля выбоки, ушедшая в левое поддерево
        p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])

        return current_gini - p * self.gini(left_labels) - (1 - p) * self.gini(right_labels)

    @staticmethod
    # Разбиение датасета в узле
    def split(data, labels, index, t):

        left = np.where(data[:, index] <= t)
        right = np.where(data[:, index] > t)

        true_data = data[left]
        false_data = data[right]
        true_labels = labels[left]
        false_labels = labels[right]

        return true_data, false_data, true_labels, false_labels

    # Нахождение наилучшего разбиения
    def find_best_split(self, data, labels, min_leaf):

        # обозначим минимальное количество объектов в узле
        # min_leaf = 5   # Вынесем переменную, как задаваемый параметр

        current_gini = self.gini(labels)

        best_quality = 0
        best_t = None
        best_index = None

        n_features = data.shape[1]

        for index in range(n_features):
            # будем проверять только уникальные значения признака, исключая повторения
            t_values = np.unique([row[index] for row in data])

            for t in t_values:
                true_data, false_data, true_labels, false_labels = self.split(data, labels, index, t)
                #  пропускаем разбиения, в которых в узле остается менее 5 объектов
                if len(true_data) < min_leaf or len(false_data) < min_leaf:
                    continue

                current_quality = self.quality(true_labels, false_labels, current_gini)

                #  выбираем порог, на котором получается максимальный прирост качества
                if current_quality > best_quality:
                    best_quality, best_t, best_index = current_quality, t, index

        return best_quality, best_t, best_index

        # Построение дерева с помощью рекурсивной функции
        # Аналог метода fit()

    def build_tree(self, data, labels, min_depth=0):

        quality, t, index = self.find_best_split(data, labels, self.min_leaf)

        #  Базовый случай - прекращаем рекурсию, когда нет прироста в качества
        if quality == 0 or min_depth > self.max_depth or self.leaves_count >= self.max_leaves-1:
            self.leaves_count += 1
            return Leaf(data, labels)

        true_data, false_data, true_labels, false_labels = self.split(data, labels, index, t)

        # Рекурсивно строим два поддерева
        true_branch = self.build_tree(true_data, true_labels, min_depth + 1)
        false_branch = self.build_tree(false_data, false_labels, min_depth + 1)

        # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева
        print(index, t, min_depth, self.leaves_count)
        return Node(index, t, true_branch, false_branch)

    def classify_object(self, obj, node):

        # Останавливаем рекурсию, если достигли листа
        if isinstance(node, Leaf):
            answer = node.prediction
            return answer

        if obj[node.index] <= node.t:
            return self.classify_object(obj, node.true_branch)
        else:
            return self.classify_object(obj, node.false_branch)

    def predict(self, data, tree_):

        classes = []
        for obj in data:
            prediction = self.classify_object(obj, tree_)
            classes.append(prediction)
        return classes


# Разобьем выборку на обучающую и тестовую
train_data, test_data, train_labels, test_labels = model_selection.train_test_split(classification_data,
                                                                                    classification_labels,
                                                                                    test_size=0.3,
                                                                                    random_state=1)

my_trees = MyDecisionTree(min_leaf=5, max_depth=5, max_leaves=1)
my_trees.build_tree(train_data, train_labels)
